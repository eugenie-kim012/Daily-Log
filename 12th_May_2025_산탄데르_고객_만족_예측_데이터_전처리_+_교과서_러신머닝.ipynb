{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtTvtYpZuAcIoNGTtAKjkZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eugenie-kim012/Daily-Log/blob/main/12th_May_2025_%EC%82%B0%ED%83%84%EB%8D%B0%EB%A5%B4_%EA%B3%A0%EA%B0%9D_%EB%A7%8C%EC%A1%B1_%EC%98%88%EC%B8%A1_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC_%2B_%EA%B5%90%EA%B3%BC%EC%84%9C_%EB%9F%AC%EC%8B%A0%EB%A8%B8%EB%8B%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "dZ9XQgJJmDat",
        "outputId": "30e99579-fd22-4d6e-fe6a-de30a97ee104"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d9970baa-be76-4f3b-839d-df822b977a94\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d9970baa-be76-4f3b-839d-df822b977a94\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train_santander.csv to train_santander (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "ASF7RgAri-AV",
        "outputId": "cda89409-b826-4115-a79a-58bcc18b46db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID  var3  var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\n",
              "0   1     2     23                 0.0                      0.0   \n",
              "1   3     2     34                 0.0                      0.0   \n",
              "2   4     2     23                 0.0                      0.0   \n",
              "\n",
              "   imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
              "0                      0.0                      0.0                      0.0   \n",
              "1                      0.0                      0.0                      0.0   \n",
              "2                      0.0                      0.0                      0.0   \n",
              "\n",
              "   imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  ...  \\\n",
              "0                      0.0                      0.0  ...   \n",
              "1                      0.0                      0.0  ...   \n",
              "2                      0.0                      0.0  ...   \n",
              "\n",
              "   saldo_medio_var33_hace2  saldo_medio_var33_hace3  saldo_medio_var33_ult1  \\\n",
              "0                      0.0                      0.0                     0.0   \n",
              "1                      0.0                      0.0                     0.0   \n",
              "2                      0.0                      0.0                     0.0   \n",
              "\n",
              "   saldo_medio_var33_ult3  saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n",
              "0                     0.0                      0.0                      0.0   \n",
              "1                     0.0                      0.0                      0.0   \n",
              "2                     0.0                      0.0                      0.0   \n",
              "\n",
              "   saldo_medio_var44_ult1  saldo_medio_var44_ult3     var38  TARGET  \n",
              "0                     0.0                     0.0  39205.17       0  \n",
              "1                     0.0                     0.0  49278.03       0  \n",
              "2                     0.0                     0.0  67333.77       0  \n",
              "\n",
              "[3 rows x 371 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb15b31c-fe12-4467-b499-4ba8cc9605d7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>var3</th>\n",
              "      <th>var15</th>\n",
              "      <th>imp_ent_var16_ult1</th>\n",
              "      <th>imp_op_var39_comer_ult1</th>\n",
              "      <th>imp_op_var39_comer_ult3</th>\n",
              "      <th>imp_op_var40_comer_ult1</th>\n",
              "      <th>imp_op_var40_comer_ult3</th>\n",
              "      <th>imp_op_var40_efect_ult1</th>\n",
              "      <th>imp_op_var40_efect_ult3</th>\n",
              "      <th>...</th>\n",
              "      <th>saldo_medio_var33_hace2</th>\n",
              "      <th>saldo_medio_var33_hace3</th>\n",
              "      <th>saldo_medio_var33_ult1</th>\n",
              "      <th>saldo_medio_var33_ult3</th>\n",
              "      <th>saldo_medio_var44_hace2</th>\n",
              "      <th>saldo_medio_var44_hace3</th>\n",
              "      <th>saldo_medio_var44_ult1</th>\n",
              "      <th>saldo_medio_var44_ult3</th>\n",
              "      <th>var38</th>\n",
              "      <th>TARGET</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39205.17</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>34</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>49278.03</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67333.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 371 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb15b31c-fe12-4467-b499-4ba8cc9605d7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bb15b31c-fe12-4467-b499-4ba8cc9605d7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bb15b31c-fe12-4467-b499-4ba8cc9605d7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-31d01e77-0964-4897-b9c4-20957fd7b7ed\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-31d01e77-0964-4897-b9c4-20957fd7b7ed')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-31d01e77-0964-4897-b9c4-20957fd7b7ed button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "cust_df"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "cust_df = pd.read_csv(\"./train_santander.csv\", encoding='latin-1')\n",
        "# encoding='latin-1'은 문자 인코딩 형식입니다. 이 파일이 UTF-8이 아닌 Latin-1 (ISO-8859-1) 형식으로 저장돼 있기 때문에 이 옵션을 줬습니다. (영어권, 유럽 데이터를 다룰 때 종종 사용됩니다.)print('dataset shape:', cust_df.shape)\n",
        "cust_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전처리"
      ],
      "metadata": {
        "id": "ITy8RUDP7NNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cust_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRBu7WM-6SUB",
        "outputId": "18aa4977-fdd9-4277-8f5c-5358095fea4f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 76020 entries, 0 to 76019\n",
            "Columns: 371 entries, ID to TARGET\n",
            "dtypes: float64(111), int64(260)\n",
            "memory usage: 215.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 만족과 불만족의 비율을 알아보기"
      ],
      "metadata": {
        "id": "UvvBK5Nb7iEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(cust_df['TARGET'].value_counts())\n",
        "unsatisfied_cnt = cust_df[cust_df['TARGET']==1].TARGET.count()\n",
        "total_cnt = cust_df.TARGET.count()\n",
        "print('unsatisfied  비율은 {0:.2f}'.format((unsatisfied_cnt / total_cnt)))\n",
        "\n",
        "#unsatisfied의 비율은 많이 높지 않음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlb3bQzg7hn9",
        "outputId": "c6d0d9f3-8016-4e7b-985e-c202780ae392"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TARGET\n",
            "0    73012\n",
            "1     3008\n",
            "Name: count, dtype: int64\n",
            "unsatisfied  비율은 0.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cust_df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "3gVKbdHr7gnl",
        "outputId": "326770c0-2171-46c0-d196-aa070afd7973"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  ID           var3         var15  imp_ent_var16_ult1  \\\n",
              "count   76020.000000   76020.000000  76020.000000        76020.000000   \n",
              "mean    75964.050723   -1523.199277     33.212865           86.208265   \n",
              "std     43781.947379   39033.462364     12.956486         1614.757313   \n",
              "min         1.000000 -999999.000000      5.000000            0.000000   \n",
              "25%     38104.750000       2.000000     23.000000            0.000000   \n",
              "50%     76043.000000       2.000000     28.000000            0.000000   \n",
              "75%    113748.750000       2.000000     40.000000            0.000000   \n",
              "max    151838.000000     238.000000    105.000000       210000.000000   \n",
              "\n",
              "       imp_op_var39_comer_ult1  imp_op_var39_comer_ult3  \\\n",
              "count             76020.000000             76020.000000   \n",
              "mean                 72.363067               119.529632   \n",
              "std                 339.315831               546.266294   \n",
              "min                   0.000000                 0.000000   \n",
              "25%                   0.000000                 0.000000   \n",
              "50%                   0.000000                 0.000000   \n",
              "75%                   0.000000                 0.000000   \n",
              "max               12888.030000             21024.810000   \n",
              "\n",
              "       imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
              "count             76020.000000             76020.000000   \n",
              "mean                  3.559130                 6.472698   \n",
              "std                  93.155749               153.737066   \n",
              "min                   0.000000                 0.000000   \n",
              "25%                   0.000000                 0.000000   \n",
              "50%                   0.000000                 0.000000   \n",
              "75%                   0.000000                 0.000000   \n",
              "max                8237.820000             11073.570000   \n",
              "\n",
              "       imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  ...  \\\n",
              "count             76020.000000             76020.000000  ...   \n",
              "mean                  0.412946                 0.567352  ...   \n",
              "std                  30.604864                36.513513  ...   \n",
              "min                   0.000000                 0.000000  ...   \n",
              "25%                   0.000000                 0.000000  ...   \n",
              "50%                   0.000000                 0.000000  ...   \n",
              "75%                   0.000000                 0.000000  ...   \n",
              "max                6600.000000              6600.000000  ...   \n",
              "\n",
              "       saldo_medio_var33_hace2  saldo_medio_var33_hace3  \\\n",
              "count             76020.000000             76020.000000   \n",
              "mean                  7.935824                 1.365146   \n",
              "std                 455.887218               113.959637   \n",
              "min                   0.000000                 0.000000   \n",
              "25%                   0.000000                 0.000000   \n",
              "50%                   0.000000                 0.000000   \n",
              "75%                   0.000000                 0.000000   \n",
              "max               50003.880000             20385.720000   \n",
              "\n",
              "       saldo_medio_var33_ult1  saldo_medio_var33_ult3  \\\n",
              "count            76020.000000            76020.000000   \n",
              "mean                12.215580                8.784074   \n",
              "std                783.207399              538.439211   \n",
              "min                  0.000000                0.000000   \n",
              "25%                  0.000000                0.000000   \n",
              "50%                  0.000000                0.000000   \n",
              "75%                  0.000000                0.000000   \n",
              "max             138831.630000            91778.730000   \n",
              "\n",
              "       saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n",
              "count             76020.000000             76020.000000   \n",
              "mean                 31.505324                 1.858575   \n",
              "std                2013.125393               147.786584   \n",
              "min                   0.000000                 0.000000   \n",
              "25%                   0.000000                 0.000000   \n",
              "50%                   0.000000                 0.000000   \n",
              "75%                   0.000000                 0.000000   \n",
              "max              438329.220000             24650.010000   \n",
              "\n",
              "       saldo_medio_var44_ult1  saldo_medio_var44_ult3         var38  \\\n",
              "count            76020.000000            76020.000000  7.602000e+04   \n",
              "mean                76.026165               56.614351  1.172358e+05   \n",
              "std               4040.337842             2852.579397  1.826646e+05   \n",
              "min                  0.000000                0.000000  5.163750e+03   \n",
              "25%                  0.000000                0.000000  6.787061e+04   \n",
              "50%                  0.000000                0.000000  1.064092e+05   \n",
              "75%                  0.000000                0.000000  1.187563e+05   \n",
              "max             681462.900000           397884.300000  2.203474e+07   \n",
              "\n",
              "             TARGET  \n",
              "count  76020.000000  \n",
              "mean       0.039569  \n",
              "std        0.194945  \n",
              "min        0.000000  \n",
              "25%        0.000000  \n",
              "50%        0.000000  \n",
              "75%        0.000000  \n",
              "max        1.000000  \n",
              "\n",
              "[8 rows x 371 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-863f3501-72da-426a-a2f0-acbd3bb76e20\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>var3</th>\n",
              "      <th>var15</th>\n",
              "      <th>imp_ent_var16_ult1</th>\n",
              "      <th>imp_op_var39_comer_ult1</th>\n",
              "      <th>imp_op_var39_comer_ult3</th>\n",
              "      <th>imp_op_var40_comer_ult1</th>\n",
              "      <th>imp_op_var40_comer_ult3</th>\n",
              "      <th>imp_op_var40_efect_ult1</th>\n",
              "      <th>imp_op_var40_efect_ult3</th>\n",
              "      <th>...</th>\n",
              "      <th>saldo_medio_var33_hace2</th>\n",
              "      <th>saldo_medio_var33_hace3</th>\n",
              "      <th>saldo_medio_var33_ult1</th>\n",
              "      <th>saldo_medio_var33_ult3</th>\n",
              "      <th>saldo_medio_var44_hace2</th>\n",
              "      <th>saldo_medio_var44_hace3</th>\n",
              "      <th>saldo_medio_var44_ult1</th>\n",
              "      <th>saldo_medio_var44_ult3</th>\n",
              "      <th>var38</th>\n",
              "      <th>TARGET</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>76020.000000</td>\n",
              "      <td>7.602000e+04</td>\n",
              "      <td>76020.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>75964.050723</td>\n",
              "      <td>-1523.199277</td>\n",
              "      <td>33.212865</td>\n",
              "      <td>86.208265</td>\n",
              "      <td>72.363067</td>\n",
              "      <td>119.529632</td>\n",
              "      <td>3.559130</td>\n",
              "      <td>6.472698</td>\n",
              "      <td>0.412946</td>\n",
              "      <td>0.567352</td>\n",
              "      <td>...</td>\n",
              "      <td>7.935824</td>\n",
              "      <td>1.365146</td>\n",
              "      <td>12.215580</td>\n",
              "      <td>8.784074</td>\n",
              "      <td>31.505324</td>\n",
              "      <td>1.858575</td>\n",
              "      <td>76.026165</td>\n",
              "      <td>56.614351</td>\n",
              "      <td>1.172358e+05</td>\n",
              "      <td>0.039569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>43781.947379</td>\n",
              "      <td>39033.462364</td>\n",
              "      <td>12.956486</td>\n",
              "      <td>1614.757313</td>\n",
              "      <td>339.315831</td>\n",
              "      <td>546.266294</td>\n",
              "      <td>93.155749</td>\n",
              "      <td>153.737066</td>\n",
              "      <td>30.604864</td>\n",
              "      <td>36.513513</td>\n",
              "      <td>...</td>\n",
              "      <td>455.887218</td>\n",
              "      <td>113.959637</td>\n",
              "      <td>783.207399</td>\n",
              "      <td>538.439211</td>\n",
              "      <td>2013.125393</td>\n",
              "      <td>147.786584</td>\n",
              "      <td>4040.337842</td>\n",
              "      <td>2852.579397</td>\n",
              "      <td>1.826646e+05</td>\n",
              "      <td>0.194945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-999999.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.163750e+03</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>38104.750000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.787061e+04</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>76043.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.064092e+05</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>113748.750000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.187563e+05</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>151838.000000</td>\n",
              "      <td>238.000000</td>\n",
              "      <td>105.000000</td>\n",
              "      <td>210000.000000</td>\n",
              "      <td>12888.030000</td>\n",
              "      <td>21024.810000</td>\n",
              "      <td>8237.820000</td>\n",
              "      <td>11073.570000</td>\n",
              "      <td>6600.000000</td>\n",
              "      <td>6600.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>50003.880000</td>\n",
              "      <td>20385.720000</td>\n",
              "      <td>138831.630000</td>\n",
              "      <td>91778.730000</td>\n",
              "      <td>438329.220000</td>\n",
              "      <td>24650.010000</td>\n",
              "      <td>681462.900000</td>\n",
              "      <td>397884.300000</td>\n",
              "      <td>2.203474e+07</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 371 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-863f3501-72da-426a-a2f0-acbd3bb76e20')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-863f3501-72da-426a-a2f0-acbd3bb76e20 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-863f3501-72da-426a-a2f0-acbd3bb76e20');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0d4d49f9-19eb-4370-85d3-cec8d8276f32\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0d4d49f9-19eb-4370-85d3-cec8d8276f32')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0d4d49f9-19eb-4370-85d3-cec8d8276f32 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 각 row은 소님이고 coloum은 항목임. 그러나 개인 정보 문제로 row 그리고 항목 모두 현재 암호화 되어 있는 상태임.\n",
        "\n",
        "#컬럼명에 포함된 키워드는 어느 정도 추측이 가능해요:\n",
        "\n",
        "접두사\t의미 (추정)\n",
        "* var\t변수 (Variable) 번호\n",
        "* imp_\t금액(Amount) 관련\n",
        "* saldo_\t잔고(Balance)\n",
        "* num_\t횟수(Count)\n",
        "* ult1, ult3\t최근 1개월, 최근 3개월 (ultimo=last)\n",
        "* comer\t상업 활동 관련 (comercial)\n",
        "* efect\t현금 관련 (efectivo)\n",
        "\n",
        "# 일단  책 필사\n"
      ],
      "metadata": {
        "id": "e7RjHJbCBqZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cust_df['var3'].replace(-999999,2,inplace=True)\n",
        "cust_df.drop('ID', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# 이 줄은 var3에서 -999999 값을 2로 대체. inplace=True를 사용하면 원본 cust_df에 바로 적용.\n",
        "#axis = 1, 해당 열을 삭제하겠다는 말 (inplace=True로 바로 적용, 해당 dataset에서 ID를 삭제하였음)\n",
        "\n",
        "#피처 세트와 레이블 세트를 분리, 레이블 칼럼은 DataFrame의 맨 마지막에 위치해 칼럼 위치 -1로 분리\n",
        "X_feactures = cust_df.iloc[:, :-1] #마지막 컬럼을 제외한 모든 열(피처들) 을 선택\n",
        "y_labels = cust_df.iloc[:, -1] #마지막 열, 즉 레이블(TARGET) 을 선택 (머신 러닝에서는 피쳐(입력 데이터)와 레이블 (정답 데이터)를 분리해야 하므로\n",
        "print('피처 데이터 shape{0}'.format(X_feactures.shape)) #치퍼의 데이터 크기를 (행, 열)로 보여주기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzv1k-0u69vB",
        "outputId": "34565bdc-1994-4d75-b40f-82672babc1c9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "피처 데이터 shape(76020, 369)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_feactures, y_labels,\n",
        "                                                    test_size=0.2, random_state=0)\n",
        "\n",
        "train_cnt = y_train.count()\n",
        "#train data만 가지고 실습하기\n",
        "test_cnt = y_test.count()\n",
        "print('학습 세트 Shape:{0}, 테스트 세트 Shape:{1}'.format(X_train.shape, X_test.shape))\n",
        "\n",
        "print('학습 세트 레이블 값 분포 비율')\n",
        "print(y_train.value_counts()/train_cnt)\n",
        "print('\\n 테스트 세트 레이블 값 분포 비율')\n",
        "print(y_test.value_counts()/test_cnt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1-QP19TBZG9",
        "outputId": "6c6ced9b-e672-40a4-cb32-a4b57b15c30a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 세트 Shape:(60816, 369), 테스트 세트 Shape:(15204, 369)\n",
            "학습 세트 레이블 값 분포 비율\n",
            "TARGET\n",
            "0    0.960964\n",
            "1    0.039036\n",
            "Name: count, dtype: float64\n",
            "\n",
            " 테스트 세트 레이블 값 분포 비율\n",
            "TARGET\n",
            "0    0.9583\n",
            "1    0.0417\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# * 학습 데이터 그리고 테스트 데이터 모두 원본 데이터와 분포가 비슷함 (전체 데이터의 4% 정도가 불만족 데이터임)"
      ],
      "metadata": {
        "id": "IhjLRalODUci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, y_train을 다시 학습과 검증 데이터 세트로 분리\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=0)"
      ],
      "metadata": {
        "id": "K-_o7SRWBYey"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBOOST (Extra Gradient boost) 모델 학습과 하이퍼 파라미터 튜닝\n",
        "* 학습 모델의 형성 및 예측 결과를 ROC AUC로 평가 (XGBClassifier의 eval_metric은 'auc'로 - 그러나 이 경우 logloss로 해도 큰 차이는 없을 것)\n",
        "* XGBCLASSIFIER를 기반으로 학습을 수행\n",
        "* n_estimator는 500으로 설정하되, early_stopping_rounds를 100 으로 설정\n",
        "* 앞에서 분리한 학습 데이터와 검증 데이터 세트를 이용, eval_set=[(X_tr, y_tr), (X_val, y_val)], 조기 중단은 100회로 설정\n",
        "* 학습을 진행한 이후, 테스트 데이터 세트로 평가된 ROC-AUC 값을 확인\n",
        "\n",
        "## https://m.blog.naver.com/snowstormaw/223219753823\n",
        "\n",
        "---\n",
        "\n",
        "GBM 파라미터와 비슷한 구조를 가짐\n",
        "\n",
        "* 일반 파라미터 - 일반적으로 실행 시 스레드 수나 silent 모드 등을 선택을 위한 파라미터로서 디폴트 파라미터 값을 바꾸는 경우는 거의 없음\n",
        "\n",
        "- booster: gbtree (디폴트) or gblinear\n",
        "- silent: 0 이 디폴트, 출력 메시지를 나타내지 않고 싶을 경우에는 1로 설정\n",
        "- nthread: CPU의 실행 스레드 개수를 조정 (드폴트 - 전체 스레드를 모두 사용)\n",
        "\n",
        "* 부스트 파라미터 - 트리 최적화, 부스팅, regularisation 등과 관련 파라미커 등을 지칭\n",
        "- eta(default=0.3, alias:learning_rate]; 디폴트는 0.1, 보통 0.01 ~0.2 선호\n",
        "- num_boost_rounds\n",
        "- min_child_weight[default=1]; 트리에서 추가적으로 가지를 나눌지 결정하기 위함. min_child_weight이 클수록 분할을 자제함 (과적합을 조절하기 위함)\n",
        "- max_depth[default=6]; 0을 지정하면 깊이에 제한이 없는 것. 일반적으로 0.5에서 1 사이.\n",
        "- colsample_bytree[default=1]; 트리 생성에 필요한 피쳐(칼럼)을 임의로 샘플링 하는데 사용. 피쳐가 많을 경우에는 과적합을 조정하는데에 적용함.\n",
        "-lamda[default=1, alias:reg_lamda]; L2 Regulation 값. 피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있음. 일반적인 과적합 방지, 계수를 줄이지만 모두 남김.\n",
        "- alpha[default=1, alias:reg_alpha]; L1 Regulation 값. 피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있음.일부 계수를 완전히 제거하기도 함.\n",
        "\n",
        "- 피처가 많고, 노이즈나 중요하지 않은 피처가 섞여 있을 때 → reg_alpha (L1) 사용\n",
        "- 과적합이 우려될 때 일반적으로 → reg_lambda (L2)\n",
        "- 두 개를 같이 조절하면서 교차 검증으로 가장 좋은 조합을 찾는 게 일반적입니다.\n",
        "\n",
        "- scale_pos_weight[default=1]: 특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 세트의 균형을 유지하기 위한 파라미터.\n",
        "\n",
        "\n",
        "* 학습 태스크 파라미터 - 학습 수행 시의 객체 함수, 평가를 위한 지표 등을 설정하는 파라미터\n",
        "- objective; 최솟값을 가져야 할 손실 함수를 정의.\n",
        "- binary:logic; 이진 분류일 때 적용\n",
        "- Multi:softmax: 다중 분류일 때 사용. 손실 함수가 Multi:softmax일 경우, 레이블 클래스의 개수인 Num_class 파라미터 정하기\n",
        "- multi_softprob\n",
        "_ eval_metrix\n",
        "\n",
        "과적합 문제가 심각하다면 고려해야 할 것\n",
        "_ eta값 낮추기 (num_round 올리기)\n",
        "_ max depth 낮추기\n",
        "_ min_child_weight값 높이기\n",
        "_gamma 값 높이기\n"
      ],
      "metadata": {
        "id": "hkfVKsylErqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GpZVo89WA9x",
        "outputId": "b75afa8f-af45-44da-f25f-bfa01919d44f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\n",
        "print(xgboost.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooJqUYhdWLf9",
        "outputId": "05d9a9a5-29b6-42b7-cc70-056f8d7dcac4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\n",
        "print(xgboost.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkpCUoTvZRit",
        "outputId": "c916c4cc-5823-4def-e3d9-f9d3c215eb23"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF3Rp13IZU-J",
        "outputId": "29fe3ad8-ebc9-4606-8abf-fb887bdd3773"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "xgb_clf = XGBClassifier(n_estimators=500, learning_rate=0.05, random_state=156, early_stopping_rounds=100, eval_metric='auc')\n",
        "xgb_clf.fit(X_tr, y_tr,\n",
        "            eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
        "            verbose=False)\n",
        "\n",
        "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1])\n",
        "print('ROC AUC: {0:.4f}'.format(xgb_roc_score))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLub7vVIAgUZ",
        "outputId": "6cf95d26-e0a3-43a8-9cbc-d7fd3cc52e70"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC: 0.8417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import hp\n",
        "\n",
        "#max_depth 는 5에서 15까지 1간격으로, min_child_weight는 1에서 6까지 1 간격으로\n",
        "#colsample_bytree는 0.5에서 0.95 사이, Learning rate는 0.01dptj 0.2사이 정규 분포된 값으로 검색\n",
        "xgb_search_space = {'max_depth': hp.quniform('max_depth', 5, 15, 1),\n",
        "                    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
        "                    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),\n",
        "                    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)}"
      ],
      "metadata": {
        "id": "nqkwkG4IZUuz"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "#fmin()에서 호출 시 search_space 값으로 XGBClassifier 교차 검증 학습 후 -1*roc_auc 평균 값을 반환\n",
        "\n",
        "def objective_func(search_space):\n",
        "  xgb_clf = XGBClassifier(n_estimators=100,\n",
        "                          max_depth=int(search_space['max_depth']),\n",
        "                          min_child_weight=int(search_space['min_child_weight']),\n",
        "                          colsample_bytree=search_space['colsample_bytree'],\n",
        "                          learning_rate=search_space['learning_rate'])\n",
        "\n",
        "  #3개 k-fold 방식으로 평가된 roc_auc 지표를 담는 list\n",
        "  roc_auc_list = []\n",
        "  #3개 k-fold 방식 적용\n",
        "  kf = KFold(n_splits=3)\n",
        "  #X_train을 다시 학습과 검증용 데이터로 분리\n",
        "  for tr_index, val_index in kf.split(X_train):\n",
        "    #kf.split(X_train)으로 추출된 학습과 검증 index 값으로 학습과 검증 데이터 세트 분리\n",
        "    X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]\n",
        "    X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
        "\n",
        "    #early stopping 값을 30회로 설정하고 추출된 학습과 검증 데이터로 XGBclassifier 수행\n",
        "    xgb_clf.fit(X_tr, y_tr,\n",
        "            eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
        "            verbose=False)\n",
        "\n",
        "    #1로 예측한 확률값 추출 후 roc auc 계산하고 평균 roc auc 계산 위해 list에 결괏값 담음\n",
        "    score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, 1])\n",
        "    roc_auc_list.append(score)\n",
        "\n",
        "    #3개 k-fold로 계산된 roc auc 값의 평균값을 반환하되\n",
        "    # Hyper0pt는 목적 함수의 최솟값을 위한 입력값을 찾으므로 -1를 곱한 뒤 반환\n",
        "    return -1 * np.mean(roc_auc_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "II3fbZCYYgk9"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import fmin, tpe, Trials\n",
        "\n",
        "trials = Trials()\n",
        "#fmin() 함수를 호출, max_evals 지정된 횟수만큼 반복 후 목적함수의 회솟값을 가지는 최적 입력값 추출,\n",
        "\n",
        "best = fmin(fn=objective_func,\n",
        "            space=xgb_search_space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=50, #최대 반복 횟수를 지정합니다\n",
        "            trials=trials, rstate=np.random.default_rng(seed=30))\n",
        "print('best:', best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56u9nTonYrVV",
        "outputId": "812ad0d3-9c03-4e70-e897-4c354408c36c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 50/50 [15:41<00:00, 18.83s/trial, best loss: -0.8349204334866185]\n",
            "best: {'colsample_bytree': np.float64(0.726055968457679), 'learning_rate': np.float64(0.051232879371870446), 'max_depth': np.float64(5.0), 'min_child_weight': np.float64(6.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#n_estimators를 500 증가 후 최적으로 찾은 하이퍼 파라미터를 기반으로 학습과 예측 수행\n",
        "xgb_clf = XGBClassifier(n_estimators=500, learning_rate=round(best['learning_rate'], 5),\n",
        "                        max_depth=int(best['max_depth']),\n",
        "                        min_child_weight=int(best['min_child_weight']),\n",
        "                        colsample_bytree=round(best['colsample_bytree'], 5))\n",
        "\n",
        "#evaluation metric를 auc로, early stopping은 100로 설정하고 학습 수행\n",
        "xgb_clf.fit(X_tr, y_tr,\n",
        "            eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
        "            verbose=False)\n",
        "\n",
        "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1])\n",
        "print('ROC AUC: {0:.4f}'.format(xgb_roc_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bah2pw7peLf",
        "outputId": "755484de-6950-4325-d4f9-f5050cda74c7"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC: 0.8420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import increment_lineno\n",
        "from xgboost import plot_importance\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "plot_importance(xgb_clf, ax=ax, max_num_features=20, height=0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "id": "GHdo0ikGvFng",
        "outputId": "f9c26c3b-1cfe-482a-a025-17bfbb5757ba"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: title={'center': 'Feature importance'}, xlabel='F score', ylabel='Features'>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgbm_clf = LGBMClassifier(n_estimators=500)\n",
        "\n",
        "eval_set = [(X_tr, y_tr), (X_val, y_val)]\n",
        "lgbm_clf.fit(X_tr, y_tr, eval_metric='auc', eval_set=eval_set)\n",
        "\n",
        "lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, 1])\n",
        "print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep6_t467vsBj",
        "outputId": "dcb0d7c8-d4e9-4371-9da7-20712d6c2543"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1658, number of negative: 40913\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076135 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 13308\n",
            "[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 242\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038947 -> initscore=-3.205836\n",
            "[LightGBM] [Info] Start training from score -3.205836\n",
            "ROC AUC: 0.8134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_search_space = {'num_leaves':hp.quniform('num_leaves', 32, 64, 1),\n",
        "                     'max_depth':hp.quniform('max_depth', 100, 160, 1),\n",
        "                     'min_child_samples':hp.quniform('min_child_samples', 60, 100, 1),\n",
        "                     'subsample':hp.uniform('subsample', 0.7, 1),\n",
        "                     'learning_rate':hp.uniform('learning_rate', 0.01, 0.2)\n",
        "                     }"
      ],
      "metadata": {
        "id": "3aNcbMPxyHcC"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "from lightgbm.callback import early_stopping\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "def objective_func(search_space):\n",
        "    lgbm_clf = LGBMClassifier(\n",
        "        n_estimators=100,\n",
        "        num_leaves=int(search_space['num_leaves']),\n",
        "        max_depth=int(search_space['max_depth']),\n",
        "        min_child_samples=int(search_space['min_child_samples']),\n",
        "        subsample=search_space['subsample'],\n",
        "        learning_rate=search_space['learning_rate']\n",
        "    )\n",
        "\n",
        "    roc_auc_list = []\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "    for tr_index, val_index in kf.split(X_train):\n",
        "        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]\n",
        "        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
        "\n",
        "        # ✅ 조기 중단 설정\n",
        "        lgbm_clf.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_metric='auc',\n",
        "            callbacks=[early_stopping(stopping_rounds=30)]\n",
        "        )\n",
        "\n",
        "        # ✅ 예측 및 평가\n",
        "        score = roc_auc_score(y_val, lgbm_clf.predict_proba(X_val)[:, 1])\n",
        "        roc_auc_list.append(score)\n",
        "\n",
        "    # ✅ 평균 AUC의 음수 반환 (hyperopt는 loss 최소화)\n",
        "    return -1 * np.mean(roc_auc_list)\n"
      ],
      "metadata": {
        "id": "GOtM3VcmzFuy"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import fmin, tpe, Trials\n",
        "trials = Trials()\n",
        "\n",
        "\n",
        "#fmin()함수를 호출, max_evals 지정된 횟수만큼 반복 후 목적함수의 회솟값을 가지는 최적 입력값을 추출\n",
        "\n",
        "best = fmin(fn=objective_func,\n",
        "            space=lgbm_search_space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=50, #최대 반복 횟수를 지정합니다\n",
        "            trials=trials, rstate=np.random.default_rng(seed=30))\n",
        "\n",
        "\n",
        "print('best:', best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aRcAgDz1Ve5",
        "outputId": "a3a5d681-605d-4538-b769-6d81d8323b46"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059255 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12868\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's auc: 0.835597\tvalid_0's binary_logloss: 0.132257\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063711 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[98]\tvalid_0's auc: 0.829021\tvalid_0's binary_logloss: 0.135222\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114611 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12933\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[99]\tvalid_0's auc: 0.839481\tvalid_0's binary_logloss: 0.13594\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064654 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12920\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[24]\tvalid_0's auc: 0.83387\tvalid_0's binary_logloss: 0.132803\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074032 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12910\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[11]\tvalid_0's auc: 0.828701\tvalid_0's binary_logloss: 0.136772\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062706 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12933\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[24]\tvalid_0's auc: 0.833999\tvalid_0's binary_logloss: 0.136863\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060293 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[40]\tvalid_0's auc: 0.835868\tvalid_0's binary_logloss: 0.132325\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093683 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[36]\tvalid_0's auc: 0.828154\tvalid_0's binary_logloss: 0.135546\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062231 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[29]\tvalid_0's auc: 0.836743\tvalid_0's binary_logloss: 0.137231\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063181 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12920\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[26]\tvalid_0's auc: 0.834069\tvalid_0's binary_logloss: 0.132662\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068017 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12947\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[21]\tvalid_0's auc: 0.825993\tvalid_0's binary_logloss: 0.136112\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079778 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12971\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[32]\tvalid_0's auc: 0.835161\tvalid_0's binary_logloss: 0.136851\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076706 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[34]\tvalid_0's auc: 0.834757\tvalid_0's binary_logloss: 0.133015\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061803 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[36]\tvalid_0's auc: 0.830658\tvalid_0's binary_logloss: 0.135194\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061923 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[63]\tvalid_0's auc: 0.838621\tvalid_0's binary_logloss: 0.135699\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061695 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's auc: 0.832981\tvalid_0's binary_logloss: 0.134702\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061352 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[99]\tvalid_0's auc: 0.827748\tvalid_0's binary_logloss: 0.137338\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099034 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12933\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[99]\tvalid_0's auc: 0.837637\tvalid_0's binary_logloss: 0.138592\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059497 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[18]\tvalid_0's auc: 0.832427\tvalid_0's binary_logloss: 0.132976\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062884 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[18]\tvalid_0's auc: 0.828402\tvalid_0's binary_logloss: 0.135577\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061539 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[17]\tvalid_0's auc: 0.83644\tvalid_0's binary_logloss: 0.136662\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065029 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12920\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's auc: 0.831548\tvalid_0's binary_logloss: 0.134178\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063999 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[15]\tvalid_0's auc: 0.827247\tvalid_0's binary_logloss: 0.13574\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093352 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12933\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[17]\tvalid_0's auc: 0.833333\tvalid_0's binary_logloss: 0.137449\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[25]\tvalid_0's auc: 0.834069\tvalid_0's binary_logloss: 0.132452\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062477 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[21]\tvalid_0's auc: 0.830247\tvalid_0's binary_logloss: 0.135256\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064528 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[22]\tvalid_0's auc: 0.837819\tvalid_0's binary_logloss: 0.135967\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064719 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[36]\tvalid_0's auc: 0.834366\tvalid_0's binary_logloss: 0.132546\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062157 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[39]\tvalid_0's auc: 0.828769\tvalid_0's binary_logloss: 0.135283\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091698 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[52]\tvalid_0's auc: 0.835877\tvalid_0's binary_logloss: 0.136278\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063697 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12955\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's auc: 0.826708\tvalid_0's binary_logloss: 0.135488\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067398 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12951\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's auc: 0.821604\tvalid_0's binary_logloss: 0.137856\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064979 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12971\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid_0's auc: 0.82961\tvalid_0's binary_logloss: 0.142835\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064876 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[42]\tvalid_0's auc: 0.835307\tvalid_0's binary_logloss: 0.132598\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062846 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[41]\tvalid_0's auc: 0.830228\tvalid_0's binary_logloss: 0.135386\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[34]\tvalid_0's auc: 0.836934\tvalid_0's binary_logloss: 0.136809\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068947 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12947\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[15]\tvalid_0's auc: 0.833032\tvalid_0's binary_logloss: 0.132914\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066813 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12951\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[13]\tvalid_0's auc: 0.825151\tvalid_0's binary_logloss: 0.136678\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067630 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12971\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[15]\tvalid_0's auc: 0.834134\tvalid_0's binary_logloss: 0.137023\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065890 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12920\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[62]\tvalid_0's auc: 0.834453\tvalid_0's binary_logloss: 0.132325\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094347 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12920\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[68]\tvalid_0's auc: 0.827755\tvalid_0's binary_logloss: 0.135496\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067786 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12971\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[87]\tvalid_0's auc: 0.837215\tvalid_0's binary_logloss: 0.13595\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062496 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12920\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[25]\tvalid_0's auc: 0.835577\tvalid_0's binary_logloss: 0.132247\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065660 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[24]\tvalid_0's auc: 0.830072\tvalid_0's binary_logloss: 0.13495\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068389 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12933\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[29]\tvalid_0's auc: 0.837488\tvalid_0's binary_logloss: 0.135898\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093533 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[29]\tvalid_0's auc: 0.834332\tvalid_0's binary_logloss: 0.133973\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063054 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[35]\tvalid_0's auc: 0.828891\tvalid_0's binary_logloss: 0.135818\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067291 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[37]\tvalid_0's auc: 0.835689\tvalid_0's binary_logloss: 0.137\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067722 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12920\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's auc: 0.835364\tvalid_0's binary_logloss: 0.132273\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068525 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[98]\tvalid_0's auc: 0.829766\tvalid_0's binary_logloss: 0.135106\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093982 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12933\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[82]\tvalid_0's auc: 0.83787\tvalid_0's binary_logloss: 0.136718\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064978 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[15]\tvalid_0's auc: 0.833915\tvalid_0's binary_logloss: 0.134197\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071099 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[26]\tvalid_0's auc: 0.828135\tvalid_0's binary_logloss: 0.135635\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071481 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12933\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[16]\tvalid_0's auc: 0.834499\tvalid_0's binary_logloss: 0.138184\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071441 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12920\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[19]\tvalid_0's auc: 0.83094\tvalid_0's binary_logloss: 0.133281\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070893 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[12]\tvalid_0's auc: 0.82462\tvalid_0's binary_logloss: 0.136567\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104421 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12933\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[18]\tvalid_0's auc: 0.831853\tvalid_0's binary_logloss: 0.137582\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070109 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12868\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's auc: 0.834948\tvalid_0's binary_logloss: 0.132411\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070599 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's auc: 0.830379\tvalid_0's binary_logloss: 0.135098\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088270 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12933\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's auc: 0.838889\tvalid_0's binary_logloss: 0.135956\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090277 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[83]\tvalid_0's auc: 0.836253\tvalid_0's binary_logloss: 0.132069\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067480 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[75]\tvalid_0's auc: 0.83205\tvalid_0's binary_logloss: 0.134831\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085930 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[99]\tvalid_0's auc: 0.83978\tvalid_0's binary_logloss: 0.135377\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070612 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[33]\tvalid_0's auc: 0.83601\tvalid_0's binary_logloss: 0.132025\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069084 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[32]\tvalid_0's auc: 0.829915\tvalid_0's binary_logloss: 0.135292\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106067 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[44]\tvalid_0's auc: 0.837971\tvalid_0's binary_logloss: 0.135773\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074992 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[74]\tvalid_0's auc: 0.834774\tvalid_0's binary_logloss: 0.132366\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070692 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[68]\tvalid_0's auc: 0.83153\tvalid_0's binary_logloss: 0.134983\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076642 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[99]\tvalid_0's auc: 0.838242\tvalid_0's binary_logloss: 0.135709\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111250 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[70]\tvalid_0's auc: 0.8364\tvalid_0's binary_logloss: 0.132067\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079999 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[72]\tvalid_0's auc: 0.83118\tvalid_0's binary_logloss: 0.134804\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071916 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[47]\tvalid_0's auc: 0.838527\tvalid_0's binary_logloss: 0.137058\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070024 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[31]\tvalid_0's auc: 0.837346\tvalid_0's binary_logloss: 0.132091\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069747 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[32]\tvalid_0's auc: 0.829886\tvalid_0's binary_logloss: 0.135238\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091720 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[44]\tvalid_0's auc: 0.837805\tvalid_0's binary_logloss: 0.135754\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069682 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's auc: 0.833925\tvalid_0's binary_logloss: 0.134994\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070738 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's auc: 0.829241\tvalid_0's binary_logloss: 0.137464\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073150 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's auc: 0.835695\tvalid_0's binary_logloss: 0.139229\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078060 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[67]\tvalid_0's auc: 0.83502\tvalid_0's binary_logloss: 0.13238\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079909 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[75]\tvalid_0's auc: 0.830728\tvalid_0's binary_logloss: 0.134932\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071425 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[81]\tvalid_0's auc: 0.83726\tvalid_0's binary_logloss: 0.13587\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072548 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[22]\tvalid_0's auc: 0.834131\tvalid_0's binary_logloss: 0.133282\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086136 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[25]\tvalid_0's auc: 0.829891\tvalid_0's binary_logloss: 0.135387\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090621 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[22]\tvalid_0's auc: 0.836182\tvalid_0's binary_logloss: 0.137111\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's auc: 0.831663\tvalid_0's binary_logloss: 0.136515\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074740 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[99]\tvalid_0's auc: 0.827093\tvalid_0's binary_logloss: 0.139149\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072300 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[97]\tvalid_0's auc: 0.837073\tvalid_0's binary_logloss: 0.140852\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073309 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[69]\tvalid_0's auc: 0.836595\tvalid_0's binary_logloss: 0.132077\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115851 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[83]\tvalid_0's auc: 0.831133\tvalid_0's binary_logloss: 0.134829\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073895 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[99]\tvalid_0's auc: 0.838679\tvalid_0's binary_logloss: 0.135517\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073205 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[37]\tvalid_0's auc: 0.836699\tvalid_0's binary_logloss: 0.132\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080484 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[44]\tvalid_0's auc: 0.832108\tvalid_0's binary_logloss: 0.134741\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075494 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[45]\tvalid_0's auc: 0.83869\tvalid_0's binary_logloss: 0.135559\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074270 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[33]\tvalid_0's auc: 0.835454\tvalid_0's binary_logloss: 0.13237\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075984 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[38]\tvalid_0's auc: 0.831394\tvalid_0's binary_logloss: 0.134958\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075566 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[45]\tvalid_0's auc: 0.838616\tvalid_0's binary_logloss: 0.135675\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075773 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[37]\tvalid_0's auc: 0.836089\tvalid_0's binary_logloss: 0.132031\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091048 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[28]\tvalid_0's auc: 0.83216\tvalid_0's binary_logloss: 0.134924\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096265 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[43]\tvalid_0's auc: 0.839124\tvalid_0's binary_logloss: 0.13562\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077067 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[23]\tvalid_0's auc: 0.836099\tvalid_0's binary_logloss: 0.132182\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081613 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[19]\tvalid_0's auc: 0.830344\tvalid_0's binary_logloss: 0.135455\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078656 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[31]\tvalid_0's auc: 0.838188\tvalid_0's binary_logloss: 0.135828\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081382 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[26]\tvalid_0's auc: 0.834535\tvalid_0's binary_logloss: 0.132471\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081335 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[26]\tvalid_0's auc: 0.828781\tvalid_0's binary_logloss: 0.135594\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119182 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[34]\tvalid_0's auc: 0.836933\tvalid_0's binary_logloss: 0.136021\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086044 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[25]\tvalid_0's auc: 0.834021\tvalid_0's binary_logloss: 0.132575\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075446 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[18]\tvalid_0's auc: 0.827557\tvalid_0's binary_logloss: 0.135827\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077978 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[29]\tvalid_0's auc: 0.834177\tvalid_0's binary_logloss: 0.136633\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090773 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[60]\tvalid_0's auc: 0.835244\tvalid_0's binary_logloss: 0.132075\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122482 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[51]\tvalid_0's auc: 0.829312\tvalid_0's binary_logloss: 0.135218\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078991 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12933\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[78]\tvalid_0's auc: 0.841022\tvalid_0's binary_logloss: 0.135107\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077851 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[20]\tvalid_0's auc: 0.833109\tvalid_0's binary_logloss: 0.132904\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090553 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[17]\tvalid_0's auc: 0.829147\tvalid_0's binary_logloss: 0.135521\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082485 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[17]\tvalid_0's auc: 0.83514\tvalid_0's binary_logloss: 0.136884\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091433 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[22]\tvalid_0's auc: 0.83326\tvalid_0's binary_logloss: 0.134418\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077373 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[23]\tvalid_0's auc: 0.829615\tvalid_0's binary_logloss: 0.13639\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080050 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[29]\tvalid_0's auc: 0.835919\tvalid_0's binary_logloss: 0.137176\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079768 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[97]\tvalid_0's auc: 0.835814\tvalid_0's binary_logloss: 0.132565\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075096 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[99]\tvalid_0's auc: 0.829936\tvalid_0's binary_logloss: 0.135443\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098805 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's auc: 0.838982\tvalid_0's binary_logloss: 0.136326\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079377 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[28]\tvalid_0's auc: 0.834359\tvalid_0's binary_logloss: 0.132431\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079533 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[16]\tvalid_0's auc: 0.828386\tvalid_0's binary_logloss: 0.135779\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088412 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[21]\tvalid_0's auc: 0.831903\tvalid_0's binary_logloss: 0.137448\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075959 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[37]\tvalid_0's auc: 0.835288\tvalid_0's binary_logloss: 0.132298\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081577 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[36]\tvalid_0's auc: 0.830516\tvalid_0's binary_logloss: 0.134773\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095084 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[44]\tvalid_0's auc: 0.838601\tvalid_0's binary_logloss: 0.135535\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079737 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[51]\tvalid_0's auc: 0.836332\tvalid_0's binary_logloss: 0.132101\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073645 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[50]\tvalid_0's auc: 0.831732\tvalid_0's binary_logloss: 0.134836\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079746 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[70]\tvalid_0's auc: 0.84014\tvalid_0's binary_logloss: 0.13523\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083901 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[74]\tvalid_0's auc: 0.836078\tvalid_0's binary_logloss: 0.131866\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076602 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[60]\tvalid_0's auc: 0.831422\tvalid_0's binary_logloss: 0.134838\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082572 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[74]\tvalid_0's auc: 0.839345\tvalid_0's binary_logloss: 0.135583\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078928 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12955\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[96]\tvalid_0's auc: 0.834765\tvalid_0's binary_logloss: 0.132429\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082290 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12995\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[96]\tvalid_0's auc: 0.828421\tvalid_0's binary_logloss: 0.13536\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094012 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12975\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[98]\tvalid_0's auc: 0.837442\tvalid_0's binary_logloss: 0.136152\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081269 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12920\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[48]\tvalid_0's auc: 0.834632\tvalid_0's binary_logloss: 0.132298\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083973 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12920\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[47]\tvalid_0's auc: 0.828512\tvalid_0's binary_logloss: 0.135197\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089568 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12971\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[65]\tvalid_0's auc: 0.838244\tvalid_0's binary_logloss: 0.135844\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112367 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[16]\tvalid_0's auc: 0.8309\tvalid_0's binary_logloss: 0.133376\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087950 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[19]\tvalid_0's auc: 0.828769\tvalid_0's binary_logloss: 0.13561\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088925 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[19]\tvalid_0's auc: 0.83384\tvalid_0's binary_logloss: 0.137321\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081019 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[86]\tvalid_0's auc: 0.834863\tvalid_0's binary_logloss: 0.133951\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[93]\tvalid_0's auc: 0.827863\tvalid_0's binary_logloss: 0.136323\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082678 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12933\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[99]\tvalid_0's auc: 0.83701\tvalid_0's binary_logloss: 0.137388\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080187 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[12]\tvalid_0's auc: 0.830468\tvalid_0's binary_logloss: 0.133907\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098002 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[14]\tvalid_0's auc: 0.828488\tvalid_0's binary_logloss: 0.135657\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090902 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[15]\tvalid_0's auc: 0.834441\tvalid_0's binary_logloss: 0.136974\n",
            "[LightGBM] [Info] Number of positive: 1601, number of negative: 38943\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097095 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12860\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039488 -> initscore=-3.191471\n",
            "[LightGBM] [Info] Start training from score -3.191471\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[84]\tvalid_0's auc: 0.836482\tvalid_0's binary_logloss: 0.131977\n",
            "[LightGBM] [Info] Number of positive: 1586, number of negative: 38958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083765 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12852\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039118 -> initscore=-3.201269\n",
            "[LightGBM] [Info] Start training from score -3.201269\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[96]\tvalid_0's auc: 0.831578\tvalid_0's binary_logloss: 0.134645\n",
            "[LightGBM] [Info] Number of positive: 1561, number of negative: 38983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078453 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12869\n",
            "[LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038501 -> initscore=-3.217799\n",
            "[LightGBM] [Info] Start training from score -3.217799\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's auc: 0.838595\tvalid_0's binary_logloss: 0.135619\n",
            "100%|██████████| 50/50 [06:26<00:00,  7.73s/trial, best loss: -0.8360679225303546]\n",
            "best: {'learning_rate': np.float64(0.050532855779112054), 'max_depth': np.float64(116.0), 'min_child_samples': np.float64(98.0), 'num_leaves': np.float64(35.0), 'subsample': np.float64(0.9553464624439237)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_clf=LGBMClassifier(n_estimators=500, num_leaves=int(best['num_leaves']),\n",
        "                        max_depth=int(best['max_depth']),\n",
        "                        min_child_samples=int(best['min_child_samples']),\n",
        "                        subsample=round(best['subsample'], 5),\n",
        "                        learning_rate=round(best['learning_rate'], 5))\n",
        "\n",
        "#evaluation metric을 auc로, early stopping 은 100로 설정하고 학습 수행\n",
        "\n",
        "\n",
        "# ✅ 조기 중단은 callbacks 로 지정\n",
        "lgbm_clf.fit(\n",
        "    X_tr, y_tr,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    callbacks=[early_stopping(stopping_rounds=100)]\n",
        ")\n",
        "\n",
        "lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, 1])\n",
        "print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0fvgNuD9qMc",
        "outputId": "c7f662e4-8938-4409-a5c6-2561dfcaa306"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1658, number of negative: 40913\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085090 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 12898\n",
            "[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 192\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038947 -> initscore=-3.205836\n",
            "[LightGBM] [Info] Start training from score -3.205836\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[69]\tvalid_0's binary_logloss: 0.134718\n",
            "ROC AUC: 0.8424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "from lightgbm.callback import early_stopping\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Hyperopt 결과로부터 최종 하이퍼파라미터 추출\n",
        "final_params = {\n",
        "    'n_estimators': 500,  # 충분히 크게 설정하고 조기 종료로 최적 지점 찾기\n",
        "    'num_leaves': int(best['num_leaves']),\n",
        "    'max_depth': int(best['max_depth']),\n",
        "    'min_child_samples': int(best['min_child_samples']),\n",
        "    'subsample': round(best['subsample'], 5),\n",
        "    'learning_rate': round(best['learning_rate'], 5),\n",
        "    'eval_metric': 'auc'  # 모델 생성 시에 넣어도 됨\n",
        "}\n",
        "\n",
        "# 모델 정의\n",
        "lgbm_clf = LGBMClassifier(**final_params)\n",
        "\n",
        "# 학습 수행 (조기 종료 포함)\n",
        "lgbm_clf.fit(\n",
        "    X_tr, y_tr,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    callbacks=[early_stopping(stopping_rounds=100)]\n",
        ")\n",
        "\n",
        "# 테스트셋 ROC AUC 평가\n",
        "lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, 1])\n",
        "print('✅ ROC AUC on Test Set: {0:.4f}'.format(lgbm_roc_score))\n",
        "\n",
        "# 최종 파라미터 및 best_iteration 출력\n",
        "print(\"\\n📌 최적 하이퍼파라미터:\")\n",
        "for k, v in final_params.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "print(f\"\\n🌟 Best iteration: {lgbm_clf.best_iteration_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpofLHR5_Xf-",
        "outputId": "91422acb-3c46-4960-ce25-0e84df6d50fb"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
            "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
            "[LightGBM] [Info] Number of positive: 1658, number of negative: 40913\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150648 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 12898\n",
            "[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 192\n",
            "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038947 -> initscore=-3.205836\n",
            "[LightGBM] [Info] Start training from score -3.205836\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[69]\tvalid_0's binary_logloss: 0.134718\n",
            "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
            "✅ ROC AUC on Test Set: 0.8424\n",
            "\n",
            "📌 최적 하이퍼파라미터:\n",
            "n_estimators: 500\n",
            "num_leaves: 35\n",
            "max_depth: 116\n",
            "min_child_samples: 98\n",
            "subsample: 0.95535\n",
            "learning_rate: 0.05053\n",
            "eval_metric: auc\n",
            "\n",
            "🌟 Best iteration: 69\n"
          ]
        }
      ]
    }
  ]
}